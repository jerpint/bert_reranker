batch_size: 2
train_file: 'train.json'
dev_file: 'dev.json'
cache_folder: 'cached_natq'
model_name: 'bert-base-uncased'
loss_type: 'classification'
freeze_bert: true
# this is mostly used when freezing the bert model
optimizer:
  name: 'adamw'
# the following allow to specify a different lr for every bert layer
#optimizer:
#  name: 'adam_diff_lr'
#  ffw_lr: 0.0
#  bert_lrs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
max_question_len: 30
max_paragraph_len: 512
top_layer_sizes: [5, 4]
patience: 5
gradient_clipping: 0
pooling_type: 'avg'
precision: 32
dropout: 0.0
normalize_bert_encoder_result: true
dropout_bert: null  # null means use the bert dropout sued during pre-training
