batch_size: 2
train_file: 'train.json'
dev_file: 'dev.json'
cache_folder: 'cached_natq'
encoder_type: 'ffw' # or use 'bert' for BERT
model_name: 'bert-base-uncased'
loss_type: 'classification'
freeze_bert: true
optimizer_type: 'adamw'
max_question_len: 30
max_paragraph_len: 512
top_layer_sizes: [5, 4]
patience: 5
gradient_clipping: 0
pooling_type: 'avg'
precision: 32
dropout: 0.0
normalize_bert_encoder_result: true
dropout_bert: null  # null means use the bert dropout used during pre-training
