train_file: 'train_small.json'
dev_files:
  dev_file_1: 'train_small.json'
  dev_file_2: 'train_small.json'
test_file: 'train_small.json'
cache_folder: 'cached_natq'
tokenizer_name: 'bert-base-uncased'

batch_size: 2
accumulate_grad_batches: 1

#model:
#  name: 'bert_encoder'
#  bert_base: 'bert-base-uncased'
#  freeze_bert: true
#  layers_pre_pooling: [5, 4]
#  layers_post_pooling: [4]
#  pooling_type: 'avg'
#  dropout: 0.0
#  normalize_model_result: true
#  dropout_bert: null  # null means use the bert dropout used during pre-training

model:
  name: 'bert_ffw'
  bert_base: 'bert-base-uncased'
  freeze_bert: true
  layers_pre_pooling: [5]
  layers_post_pooling: [5]
  pooling_type: 'avg'
  dropout: 0.0
  normalize_model_result: true
  retriever_layer_sizes: [6]
  dropout_bert: null  # null means use the bert dropout used during pre-training

# cnn example - still not working
#  name: 'cnn'
#  cnn_layer_sizes: [24]
#  emb_size: 300
#  layers_pre_pooling: []
#  layers_post_pooling: [4]
#  pooling_type: 'avg'
#  dropout: 0.0
#  normalize_model_result: true

loss_type: 'classification'

# this is mostly used when freezing the bert model
optimizer:
  name: 'adamw'
  lr: 0.001
# the following allow to specify a different lr for every bert layer
#optimizer:
#  name: 'adam_diff_lr'
#  ffw_lr: 0.0
#  bert_lrs: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001]

max_question_len: 30
max_paragraph_len: 512
patience: 5
max_epochs: 1
gradient_clipping: 0
precision: 32
seed: null